vocabulary size: 74
n tunes: 375
n train tunes: 311
n validation tunes: 64
min, max length 44 1047
Train model
Embedding shape (64, ?, 74)
RNN output shape (64, ?, 512)
Reshape shape (?, 512)
Dense shape (?, 74)
0/485 (epoch 0.000) train_loss=647.44616699 time/batch=9.81s
1/485 (epoch 0.206) train_loss=831.51428223 time/batch=8.91s
2/485 (epoch 0.412) train_loss=1085.11047363 time/batch=17.27s
3/485 (epoch 0.617) train_loss=430.72451782 time/batch=4.43s
4/485 (epoch 0.823) train_loss=420.78649902 time/batch=4.35s
5/485 (epoch 1.029) train_loss=780.72149658 time/batch=7.75s
6/485 (epoch 1.235) train_loss=935.81384277 time/batch=12.35s
7/485 (epoch 1.441) train_loss=547.50836182 time/batch=6.30s
8/485 (epoch 1.646) train_loss=419.43328857 time/batch=5.34s
9/485 (epoch 1.852) train_loss=595.49224854 time/batch=6.19s
10/485 (epoch 2.058) train_loss=1329.26171875 time/batch=45.62s
11/485 (epoch 2.264) train_loss=614.65820312 time/batch=6.96s
12/485 (epoch 2.469) train_loss=417.98681641 time/batch=4.90s
13/485 (epoch 2.675) train_loss=687.09460449 time/batch=6.87s
14/485 (epoch 2.881) train_loss=916.98071289 time/batch=10.63s
15/485 (epoch 3.087) train_loss=1136.44018555 time/batch=57.55s
16/485 (epoch 3.293) train_loss=561.04876709 time/batch=5.62s
17/485 (epoch 3.498) train_loss=416.05178833 time/batch=4.79s
18/485 (epoch 3.704) train_loss=731.07690430 time/batch=8.14s
19/485 (epoch 3.910) train_loss=1318.05383301 time/batch=52.90s
20/485 (epoch 4.116) train_loss=874.31225586 time/batch=10.37s
21/485 (epoch 4.322) train_loss=577.36486816 time/batch=5.68s
22/485 (epoch 4.527) train_loss=548.40368652 time/batch=6.55s
23/485 (epoch 4.733) train_loss=896.64428711 time/batch=17.33s
24/485 (epoch 4.939) train_loss=730.77770996 time/batch=8.29s
25/485 (epoch 5.145) train_loss=485.04473877 time/batch=5.77s
26/485 (epoch 5.350) train_loss=1306.92773438 time/batch=54.97s
27/485 (epoch 5.556) train_loss=462.87933350 time/batch=6.06s
28/485 (epoch 5.762) train_loss=449.09262085 time/batch=4.77s
29/485 (epoch 5.968) train_loss=495.06597900 time/batch=5.90s
30/485 (epoch 6.174) train_loss=952.57775879 time/batch=12.62s
31/485 (epoch 6.379) train_loss=1098.89965820 time/batch=55.72s
32/485 (epoch 6.585) train_loss=686.62158203 time/batch=8.27s
33/485 (epoch 6.791) train_loss=1291.38891602 time/batch=209.18s
34/485 (epoch 6.997) train_loss=405.61410522 time/batch=113.04s
35/485 (epoch 7.203) train_loss=527.30786133 time/batch=7.34s
36/485 (epoch 7.408) train_loss=465.46554565 time/batch=5.75s
37/485 (epoch 7.614) train_loss=568.95233154 time/batch=7.30s
38/485 (epoch 7.820) train_loss=1091.98510742 time/batch=23.26s
39/485 (epoch 8.026) train_loss=526.47344971 time/batch=8.81s
40/485 (epoch 8.232) train_loss=493.82604980 time/batch=7.20s
41/485 (epoch 8.437) train_loss=931.09948730 time/batch=22.24s
42/485 (epoch 8.643) train_loss=1055.75085449 time/batch=79.41s
43/485 (epoch 8.849) train_loss=605.59814453 time/batch=9.77s
44/485 (epoch 9.055) train_loss=425.45013428 time/batch=9.68s
45/485 (epoch 9.260) train_loss=1224.04504395 time/batch=78.21s
46/485 (epoch 9.466) train_loss=653.65863037 time/batch=11.68s
47/485 (epoch 9.672) train_loss=452.44107056 time/batch=9.10s
48/485 (epoch 9.878) train_loss=484.69122314 time/batch=9.18s
49/485 (epoch 10.084) train_loss=1146.94042969 time/batch=76.58s
50/485 (epoch 10.289) train_loss=629.73229980 time/batch=14.40s
51/485 (epoch 10.495) train_loss=514.23693848 time/batch=13.90s
52/485 (epoch 10.701) train_loss=670.91259766 time/batch=17.19s
53/485 (epoch 10.907) train_loss=478.30255127 time/batch=11.51s
54/485 (epoch 11.113) train_loss=1052.69775391 time/batch=92.72s
55/485 (epoch 11.318) train_loss=462.80245972 time/batch=12.04s
56/485 (epoch 11.524) train_loss=399.66342163 time/batch=10.00s
57/485 (epoch 11.730) train_loss=1020.23571777 time/batch=65.53s
58/485 (epoch 11.936) train_loss=676.78112793 time/batch=99.19s
59/485 (epoch 12.141) train_loss=392.51422119 time/batch=12.16s
60/485 (epoch 12.347) train_loss=862.78234863 time/batch=35.40s
61/485 (epoch 12.553) train_loss=331.79330444 time/batch=9.52s
62/485 (epoch 12.759) train_loss=821.79736328 time/batch=107.91s
63/485 (epoch 12.965) train_loss=507.27267456 time/batch=17.16s
64/485 (epoch 13.170) train_loss=564.99804688 time/batch=16.88s
65/485 (epoch 13.376) train_loss=750.90118408 time/batch=26.00s
66/485 (epoch 13.582) train_loss=346.78509521 time/batch=10.23s
67/485 (epoch 13.788) train_loss=875.25097656 time/batch=108.39s
68/485 (epoch 13.994) train_loss=411.48004150 time/batch=13.08s
69/485 (epoch 14.199) train_loss=724.99774170 time/batch=25.25s
70/485 (epoch 14.405) train_loss=351.29199219 time/batch=14.03s
71/485 (epoch 14.611) train_loss=602.49468994 time/batch=28.52s
72/485 (epoch 14.817) train_loss=474.71670532 time/batch=15.73s
73/485 (epoch 15.023) train_loss=1068.97546387 time/batch=117.99s
74/485 (epoch 15.228) train_loss=555.23089600 time/batch=18.08s
75/485 (epoch 15.434) train_loss=329.75665283 time/batch=11.24s
76/485 (epoch 15.640) train_loss=1054.63964844 time/batch=127.40s
77/485 (epoch 15.846) train_loss=399.70202637 time/batch=15.77s
78/485 (epoch 16.051) train_loss=562.75677490 time/batch=22.32s
79/485 (epoch 16.257) train_loss=452.30422974 time/batch=18.09s
  saved to metadata
80/485 (epoch 16.463) train_loss=327.23312378 time/batch=13.82s
81/485 (epoch 16.669) train_loss=531.40936279 time/batch=19.98s
82/485 (epoch 16.875) train_loss=1065.63354492 time/batch=156.70s
83/485 (epoch 17.080) train_loss=595.39514160 time/batch=31.47s
84/485 (epoch 17.286) train_loss=383.56542969 time/batch=13.90s
85/485 (epoch 17.492) train_loss=457.10241699 time/batch=25.12s
86/485 (epoch 17.698) train_loss=761.95855713 time/batch=38.81s
87/485 (epoch 17.904) train_loss=709.74511719 time/batch=45.90s
setting learning rate to 0.0029100
88/485 (epoch 18.109) train_loss=502.26370239 time/batch=19.49s
89/485 (epoch 18.315) train_loss=646.95336914 time/batch=26.87s
90/485 (epoch 18.521) train_loss=1024.31872559 time/batch=155.47s
91/485 (epoch 18.727) train_loss=331.01135254 time/batch=14.55s
setting learning rate to 0.0028227
92/485 (epoch 18.932) train_loss=325.70883179 time/batch=17.22s
93/485 (epoch 19.138) train_loss=1078.22546387 time/batch=195.33s
94/485 (epoch 19.344) train_loss=545.24426270 time/batch=28.28s
95/485 (epoch 19.550) train_loss=421.04864502 time/batch=22.53s
setting learning rate to 0.0027380
96/485 (epoch 19.756) train_loss=780.84960938 time/batch=50.06s
97/485 (epoch 19.961) train_loss=444.01367188 time/batch=22.30s
98/485 (epoch 20.167) train_loss=417.16424561 time/batch=23.78s
99/485 (epoch 20.373) train_loss=428.80181885 time/batch=27.40s
setting learning rate to 0.0026559
100/485 (epoch 20.579) train_loss=907.49584961 time/batch=75.44s
101/485 (epoch 20.785) train_loss=422.58306885 time/batch=21.33s
102/485 (epoch 20.990) train_loss=338.86160278 time/batch=25.12s
103/485 (epoch 21.196) train_loss=773.87084961 time/batch=184.71s
setting learning rate to 0.0025762
104/485 (epoch 21.402) train_loss=1049.48901367 time/batch=195.18s
105/485 (epoch 21.608) train_loss=428.76000977 time/batch=21.69s
106/485 (epoch 21.814) train_loss=325.61517334 time/batch=18.26s
107/485 (epoch 22.019) train_loss=564.03796387 time/batch=28.35s
setting learning rate to 0.0024989
108/485 (epoch 22.225) train_loss=324.47375488 time/batch=17.28s
109/485 (epoch 22.431) train_loss=833.17718506 time/batch=52.41s
110/485 (epoch 22.637) train_loss=420.67147827 time/batch=21.29s
111/485 (epoch 22.842) train_loss=644.55340576 time/batch=56.96s
setting learning rate to 0.0024239
112/485 (epoch 23.048) train_loss=454.77560425 time/batch=18.92s
113/485 (epoch 23.254) train_loss=374.38983154 time/batch=19.34s
114/485 (epoch 23.460) train_loss=847.71075439 time/batch=58.27s
115/485 (epoch 23.666) train_loss=421.03182983 time/batch=22.58s
setting learning rate to 0.0023512
116/485 (epoch 23.871) train_loss=433.42138672 time/batch=17.55s
117/485 (epoch 24.077) train_loss=775.14709473 time/batch=33.92s
118/485 (epoch 24.283) train_loss=327.90435791 time/batch=12.70s
119/485 (epoch 24.489) train_loss=628.49334717 time/batch=37.69s
setting learning rate to 0.0022807
120/485 (epoch 24.695) train_loss=504.75137329 time/batch=18.35s
121/485 (epoch 24.900) train_loss=601.73156738 time/batch=23.42s
122/485 (epoch 25.106) train_loss=1052.93383789 time/batch=140.81s
123/485 (epoch 25.312) train_loss=399.33868408 time/batch=17.24s
setting learning rate to 0.0022123
124/485 (epoch 25.518) train_loss=508.22738647 time/batch=26.13s
125/485 (epoch 25.723) train_loss=455.21377563 time/batch=22.89s
126/485 (epoch 25.929) train_loss=419.77828979 time/batch=22.96s
127/485 (epoch 26.135) train_loss=954.45916748 time/batch=65.65s
setting learning rate to 0.0021459
128/485 (epoch 26.341) train_loss=405.26232910 time/batch=19.71s
129/485 (epoch 26.547) train_loss=602.79614258 time/batch=34.92s
130/485 (epoch 26.752) train_loss=344.75094604 time/batch=17.86s
131/485 (epoch 26.958) train_loss=657.09661865 time/batch=35.52s
setting learning rate to 0.0020815
132/485 (epoch 27.164) train_loss=710.72949219 time/batch=39.62s
133/485 (epoch 27.370) train_loss=357.41140747 time/batch=18.46s
134/485 (epoch 27.576) train_loss=387.30474854 time/batch=22.97s
135/485 (epoch 27.781) train_loss=950.59326172 time/batch=193.32s
setting learning rate to 0.0020191
136/485 (epoch 27.987) train_loss=1040.08544922 time/batch=151.87s
137/485 (epoch 28.193) train_loss=378.57708740 time/batch=15.58s
138/485 (epoch 28.399) train_loss=361.75708008 time/batch=18.79s
139/485 (epoch 28.605) train_loss=885.69274902 time/batch=21.01s
setting learning rate to 0.0019585
140/485 (epoch 28.810) train_loss=878.40985107 time/batch=50.16s
141/485 (epoch 29.016) train_loss=1732.33569336 time/batch=141.80s
142/485 (epoch 29.222) train_loss=324.02490234 time/batch=14.01s
143/485 (epoch 29.428) train_loss=483.16223145 time/batch=19.50s
setting learning rate to 0.0018998
144/485 (epoch 29.633) train_loss=345.57434082 time/batch=14.28s
145/485 (epoch 29.839) train_loss=568.13507080 time/batch=23.38s
146/485 (epoch 30.045) train_loss=1049.23925781 time/batch=141.80s
147/485 (epoch 30.251) train_loss=540.21105957 time/batch=25.45s
setting learning rate to 0.0018428
148/485 (epoch 30.457) train_loss=764.64123535 time/batch=38.32s
149/485 (epoch 30.662) train_loss=498.38818359 time/batch=20.51s
150/485 (epoch 30.868) train_loss=324.07330322 time/batch=14.12s
151/485 (epoch 31.074) train_loss=503.57742310 time/batch=23.64s
setting learning rate to 0.0017875
152/485 (epoch 31.280) train_loss=1034.82177734 time/batch=151.28s
153/485 (epoch 31.486) train_loss=373.38775635 time/batch=15.43s
154/485 (epoch 31.691) train_loss=360.78393555 time/batch=17.66s
155/485 (epoch 31.897) train_loss=606.57580566 time/batch=27.52s
setting learning rate to 0.0017339
156/485 (epoch 32.103) train_loss=560.11798096 time/batch=25.37s
157/485 (epoch 32.309) train_loss=624.54272461 time/batch=29.53s
158/485 (epoch 32.514) train_loss=363.36541748 time/batch=14.21s
159/485 (epoch 32.720) train_loss=627.65173340 time/batch=48.75s
setting learning rate to 0.0016818
  saved to metadata
160/485 (epoch 32.926) train_loss=506.58380127 time/batch=25.85s
161/485 (epoch 33.132) train_loss=510.06970215 time/batch=24.41s
162/485 (epoch 33.338) train_loss=400.94284058 time/batch=27.09s
163/485 (epoch 33.543) train_loss=445.69052124 time/batch=32.63s
setting learning rate to 0.0016314
164/485 (epoch 33.749) train_loss=355.57342529 time/batch=17.04s
165/485 (epoch 33.955) train_loss=459.83233643 time/batch=19.94s
166/485 (epoch 34.161) train_loss=1019.79663086 time/batch=170.95s
167/485 (epoch 34.367) train_loss=600.82006836 time/batch=33.24s
setting learning rate to 0.0015824
168/485 (epoch 34.572) train_loss=611.81304932 time/batch=36.45s
169/485 (epoch 34.778) train_loss=330.00695801 time/batch=16.78s
170/485 (epoch 34.984) train_loss=657.38183594 time/batch=41.59s
171/485 (epoch 35.190) train_loss=457.12121582 time/batch=23.21s
setting learning rate to 0.0015350
172/485 (epoch 35.395) train_loss=621.50646973 time/batch=35.46s
173/485 (epoch 35.601) train_loss=982.15344238 time/batch=168.71s
174/485 (epoch 35.807) train_loss=342.82424927 time/batch=17.17s
175/485 (epoch 36.013) train_loss=456.81805420 time/batch=25.03s
setting learning rate to 0.0014889
176/485 (epoch 36.219) train_loss=1010.40148926 time/batch=163.78s
177/485 (epoch 36.424) train_loss=318.60543823 time/batch=16.89s
178/485 (epoch 36.630) train_loss=574.81994629 time/batch=30.56s
179/485 (epoch 36.836) train_loss=409.70877075 time/batch=23.69s
setting learning rate to 0.0014443
180/485 (epoch 37.042) train_loss=1005.87768555 time/batch=357.57s
181/485 (epoch 37.248) train_loss=496.41781616 time/batch=22.86s
182/485 (epoch 37.453) train_loss=317.60980225 time/batch=16.39s
183/485 (epoch 37.659) train_loss=582.71783447 time/batch=35.63s
setting learning rate to 0.0014009
184/485 (epoch 37.865) train_loss=422.94653320 time/batch=22.04s
185/485 (epoch 38.071) train_loss=619.96527100 time/batch=31.15s
186/485 (epoch 38.277) train_loss=679.34423828 time/batch=43.03s
187/485 (epoch 38.482) train_loss=769.52716064 time/batch=158.40s
setting learning rate to 0.0013589
188/485 (epoch 38.688) train_loss=736.56726074 time/batch=41.05s
189/485 (epoch 38.894) train_loss=519.57116699 time/batch=22.56s
190/485 (epoch 39.100) train_loss=365.75366211 time/batch=17.70s
191/485 (epoch 39.305) train_loss=814.88189697 time/batch=212.41s
setting learning rate to 0.0013181
192/485 (epoch 39.511) train_loss=625.27087402 time/batch=43.94s
193/485 (epoch 39.717) train_loss=358.91244507 time/batch=20.84s
194/485 (epoch 39.923) train_loss=958.25311279 time/batch=189.04s
195/485 (epoch 40.129) train_loss=441.37658691 time/batch=22.91s
setting learning rate to 0.0012786
196/485 (epoch 40.334) train_loss=999.74768066 time/batch=200.11s
197/485 (epoch 40.540) train_loss=426.11019897 time/batch=21.38s
198/485 (epoch 40.746) train_loss=502.26550293 time/batch=26.27s
199/485 (epoch 40.952) train_loss=453.21200562 time/batch=27.19s
setting learning rate to 0.0012402
200/485 (epoch 41.158) train_loss=518.99176025 time/batch=26.73s
201/485 (epoch 41.363) train_loss=995.62695312 time/batch=197.15s
202/485 (epoch 41.569) train_loss=315.06262207 time/batch=21.23s
203/485 (epoch 41.775) train_loss=529.22106934 time/batch=41.45s
setting learning rate to 0.0012030
204/485 (epoch 41.981) train_loss=786.04785156 time/batch=80.55s
205/485 (epoch 42.186) train_loss=425.90863037 time/batch=27.55s
206/485 (epoch 42.392) train_loss=535.43615723 time/batch=32.37s
207/485 (epoch 42.598) train_loss=317.59545898 time/batch=20.64s
setting learning rate to 0.0011669
208/485 (epoch 42.804) train_loss=593.78503418 time/batch=35.83s
209/485 (epoch 43.010) train_loss=464.42861938 time/batch=27.18s
210/485 (epoch 43.215) train_loss=759.48675537 time/batch=48.29s
211/485 (epoch 43.421) train_loss=684.24316406 time/batch=183.80s
setting learning rate to 0.0011319
212/485 (epoch 43.627) train_loss=424.49819946 time/batch=20.53s
213/485 (epoch 43.833) train_loss=502.63629150 time/batch=24.29s
214/485 (epoch 44.039) train_loss=561.72290039 time/batch=30.88s
215/485 (epoch 44.244) train_loss=346.31890869 time/batch=31.96s
setting learning rate to 0.0010980
216/485 (epoch 44.450) train_loss=520.85992432 time/batch=25.15s
217/485 (epoch 44.656) train_loss=537.59106445 time/batch=30.26s
218/485 (epoch 44.862) train_loss=548.84289551 time/batch=40.37s
219/485 (epoch 45.068) train_loss=850.50024414 time/batch=197.36s
setting learning rate to 0.0010650
220/485 (epoch 45.273) train_loss=549.04345703 time/batch=31.01s
221/485 (epoch 45.479) train_loss=450.95709229 time/batch=22.11s
222/485 (epoch 45.685) train_loss=513.54284668 time/batch=32.11s
223/485 (epoch 45.891) train_loss=942.14379883 time/batch=167.16s
setting learning rate to 0.0010331
224/485 (epoch 46.096) train_loss=981.64831543 time/batch=176.03s
225/485 (epoch 46.302) train_loss=606.40026855 time/batch=29.95s
226/485 (epoch 46.508) train_loss=359.54046631 time/batch=17.13s
227/485 (epoch 46.714) train_loss=438.67553711 time/batch=21.98s
setting learning rate to 0.0010021
228/485 (epoch 46.920) train_loss=316.38781738 time/batch=16.32s
229/485 (epoch 47.125) train_loss=630.62353516 time/batch=33.78s
230/485 (epoch 47.331) train_loss=455.26879883 time/batch=22.49s
231/485 (epoch 47.537) train_loss=956.69738770 time/batch=174.87s
setting learning rate to 0.0009720
232/485 (epoch 47.743) train_loss=346.36145020 time/batch=16.89s
233/485 (epoch 47.949) train_loss=987.83319092 time/batch=183.21s
234/485 (epoch 48.154) train_loss=393.10382080 time/batch=22.40s
235/485 (epoch 48.360) train_loss=477.09259033 time/batch=25.31s
setting learning rate to 0.0009429
236/485 (epoch 48.566) train_loss=738.68212891 time/batch=52.56s
237/485 (epoch 48.772) train_loss=833.86682129 time/batch=205.24s
238/485 (epoch 48.977) train_loss=447.93066406 time/batch=23.10s
239/485 (epoch 49.183) train_loss=314.20046997 time/batch=16.99s
setting learning rate to 0.0009146
  saved to metadata
240/485 (epoch 49.389) train_loss=442.65859985 time/batch=23.46s
241/485 (epoch 49.595) train_loss=915.46276855 time/batch=134.34s
242/485 (epoch 49.801) train_loss=343.94000244 time/batch=18.40s
243/485 (epoch 50.006) train_loss=547.84411621 time/batch=33.24s
setting learning rate to 0.0008871
244/485 (epoch 50.212) train_loss=734.55505371 time/batch=50.19s
245/485 (epoch 50.418) train_loss=588.32983398 time/batch=52.20s
246/485 (epoch 50.624) train_loss=455.70471191 time/batch=23.44s
247/485 (epoch 50.830) train_loss=375.76556396 time/batch=19.75s
setting learning rate to 0.0008605
248/485 (epoch 51.035) train_loss=464.00332642 time/batch=24.77s
249/485 (epoch 51.241) train_loss=987.33227539 time/batch=182.69s
250/485 (epoch 51.447) train_loss=561.72949219 time/batch=30.89s
251/485 (epoch 51.653) train_loss=354.16406250 time/batch=18.27s
setting learning rate to 0.0008347
252/485 (epoch 51.859) train_loss=416.40307617 time/batch=22.78s
253/485 (epoch 52.064) train_loss=461.57192993 time/batch=26.19s
254/485 (epoch 52.270) train_loss=484.56817627 time/batch=29.45s
255/485 (epoch 52.476) train_loss=981.91339111 time/batch=197.50s
setting learning rate to 0.0008097
256/485 (epoch 52.682) train_loss=440.14401245 time/batch=23.33s
257/485 (epoch 52.887) train_loss=334.04882812 time/batch=17.32s
258/485 (epoch 53.093) train_loss=564.21069336 time/batch=30.40s
259/485 (epoch 53.299) train_loss=479.64166260 time/batch=34.34s
setting learning rate to 0.0007854
260/485 (epoch 53.505) train_loss=976.53967285 time/batch=186.53s
261/485 (epoch 53.711) train_loss=312.71978760 time/batch=16.91s
262/485 (epoch 53.916) train_loss=473.82891846 time/batch=24.55s
263/485 (epoch 54.122) train_loss=418.02822876 time/batch=24.93s
setting learning rate to 0.0007618
264/485 (epoch 54.328) train_loss=589.63781738 time/batch=30.80s
265/485 (epoch 54.534) train_loss=611.48193359 time/batch=39.67s
266/485 (epoch 54.740) train_loss=313.33139038 time/batch=17.47s
267/485 (epoch 54.945) train_loss=829.22949219 time/batch=187.23s
setting learning rate to 0.0007390
268/485 (epoch 55.151) train_loss=510.47351074 time/batch=26.36s
269/485 (epoch 55.357) train_loss=832.17321777 time/batch=72.61s
270/485 (epoch 55.563) train_loss=347.35107422 time/batch=18.60s
271/485 (epoch 55.768) train_loss=615.14807129 time/batch=126.15s
setting learning rate to 0.0007168
272/485 (epoch 55.974) train_loss=535.22192383 time/batch=27.83s
273/485 (epoch 56.180) train_loss=931.09411621 time/batch=131.95s
274/485 (epoch 56.386) train_loss=440.21743774 time/batch=24.75s
275/485 (epoch 56.592) train_loss=310.59051514 time/batch=17.62s
setting learning rate to 0.0006953
276/485 (epoch 56.797) train_loss=743.12066650 time/batch=46.92s
277/485 (epoch 57.003) train_loss=312.17147827 time/batch=16.74s
278/485 (epoch 57.209) train_loss=802.66693115 time/batch=175.08s
279/485 (epoch 57.415) train_loss=406.37274170 time/batch=21.23s
setting learning rate to 0.0006744
280/485 (epoch 57.621) train_loss=968.12365723 time/batch=175.98s
281/485 (epoch 57.826) train_loss=562.93768311 time/batch=30.35s
282/485 (epoch 58.032) train_loss=443.04833984 time/batch=23.72s
283/485 (epoch 58.238) train_loss=333.82727051 time/batch=17.02s
setting learning rate to 0.0006542
284/485 (epoch 58.444) train_loss=514.43927002 time/batch=26.48s
285/485 (epoch 58.650) train_loss=954.33715820 time/batch=177.17s
286/485 (epoch 58.855) train_loss=540.46185303 time/batch=31.41s
287/485 (epoch 59.061) train_loss=375.13574219 time/batch=18.38s
setting learning rate to 0.0006346
288/485 (epoch 59.267) train_loss=523.56134033 time/batch=27.30s
289/485 (epoch 59.473) train_loss=311.51281738 time/batch=16.94s
290/485 (epoch 59.678) train_loss=559.18835449 time/batch=34.17s
291/485 (epoch 59.884) train_loss=403.56756592 time/batch=21.06s
setting learning rate to 0.0006155
292/485 (epoch 60.090) train_loss=703.42407227 time/batch=42.96s
293/485 (epoch 60.296) train_loss=796.61309814 time/batch=131.73s
294/485 (epoch 60.502) train_loss=447.85079956 time/batch=23.37s
295/485 (epoch 60.707) train_loss=397.65621948 time/batch=24.34s
setting learning rate to 0.0005971
296/485 (epoch 60.913) train_loss=363.43225098 time/batch=18.20s
297/485 (epoch 61.119) train_loss=963.00592041 time/batch=180.98s
298/485 (epoch 61.325) train_loss=558.57019043 time/batch=30.41s
299/485 (epoch 61.531) train_loss=493.31188965 time/batch=31.36s
setting learning rate to 0.0005792
300/485 (epoch 61.736) train_loss=339.84954834 time/batch=17.22s
301/485 (epoch 61.942) train_loss=531.78088379 time/batch=27.96s
302/485 (epoch 62.148) train_loss=429.67254639 time/batch=22.41s
303/485 (epoch 62.354) train_loss=732.69207764 time/batch=47.21s
setting learning rate to 0.0005618
304/485 (epoch 62.559) train_loss=314.82916260 time/batch=17.02s
305/485 (epoch 62.765) train_loss=691.71020508 time/batch=43.02s
306/485 (epoch 62.971) train_loss=837.31903076 time/batch=179.95s
307/485 (epoch 63.177) train_loss=403.47094727 time/batch=21.19s
setting learning rate to 0.0005449
308/485 (epoch 63.383) train_loss=542.39099121 time/batch=29.12s
309/485 (epoch 63.588) train_loss=343.49569702 time/batch=18.18s
310/485 (epoch 63.794) train_loss=512.12695312 time/batch=32.40s
311/485 (epoch 64.000) train_loss=840.01593018 time/batch=77.86s
setting learning rate to 0.0005286
312/485 (epoch 64.206) train_loss=707.69641113 time/batch=46.04s
313/485 (epoch 64.412) train_loss=546.48632812 time/batch=46.41s
314/485 (epoch 64.617) train_loss=535.43908691 time/batch=62.74s
315/485 (epoch 64.823) train_loss=414.61871338 time/batch=69.37s
setting learning rate to 0.0005127
316/485 (epoch 65.029) train_loss=308.62158203 time/batch=16.90s
317/485 (epoch 65.235) train_loss=449.64431763 time/batch=23.70s
318/485 (epoch 65.441) train_loss=947.11926270 time/batch=180.92s
319/485 (epoch 65.646) train_loss=551.62896729 time/batch=30.06s
setting learning rate to 0.0004973
  saved to metadata
320/485 (epoch 65.852) train_loss=619.18853760 time/batch=37.29s
321/485 (epoch 66.058) train_loss=369.52093506 time/batch=18.67s
322/485 (epoch 66.264) train_loss=341.59002686 time/batch=21.36s
323/485 (epoch 66.469) train_loss=647.33618164 time/batch=47.63s
setting learning rate to 0.0004824
324/485 (epoch 66.675) train_loss=942.91571045 time/batch=182.01s
325/485 (epoch 66.881) train_loss=362.99139404 time/batch=18.24s
326/485 (epoch 67.087) train_loss=533.09179688 time/batch=29.36s
327/485 (epoch 67.293) train_loss=368.13568115 time/batch=21.66s
setting learning rate to 0.0004679
328/485 (epoch 67.498) train_loss=537.28784180 time/batch=29.12s
329/485 (epoch 67.704) train_loss=471.86975098 time/batch=30.03s
330/485 (epoch 67.910) train_loss=943.14459229 time/batch=183.53s
331/485 (epoch 68.116) train_loss=308.53564453 time/batch=17.18s
setting learning rate to 0.0004539
332/485 (epoch 68.322) train_loss=378.59478760 time/batch=19.47s
333/485 (epoch 68.527) train_loss=521.14154053 time/batch=28.04s
334/485 (epoch 68.733) train_loss=458.89004517 time/batch=29.45s
335/485 (epoch 68.939) train_loss=860.58862305 time/batch=79.70s
setting learning rate to 0.0004403
336/485 (epoch 69.145) train_loss=946.10778809 time/batch=184.40s
337/485 (epoch 69.350) train_loss=492.96493530 time/batch=25.87s
338/485 (epoch 69.556) train_loss=306.73025513 time/batch=17.31s
339/485 (epoch 69.762) train_loss=399.76080322 time/batch=21.53s
setting learning rate to 0.0004271
340/485 (epoch 69.968) train_loss=576.10601807 time/batch=32.71s
341/485 (epoch 70.174) train_loss=677.97363281 time/batch=46.83s
342/485 (epoch 70.379) train_loss=472.98483276 time/batch=50.16s
343/485 (epoch 70.585) train_loss=500.24118042 time/batch=71.83s
setting learning rate to 0.0004143
344/485 (epoch 70.791) train_loss=755.86376953 time/batch=59.58s
345/485 (epoch 70.997) train_loss=555.70440674 time/batch=62.31s
346/485 (epoch 71.203) train_loss=446.39859009 time/batch=24.15s
347/485 (epoch 71.408) train_loss=316.43298340 time/batch=17.49s
setting learning rate to 0.0004018
348/485 (epoch 71.614) train_loss=698.68395996 time/batch=46.33s
349/485 (epoch 71.820) train_loss=353.03286743 time/batch=18.30s
350/485 (epoch 72.026) train_loss=814.21063232 time/batch=186.02s
351/485 (epoch 72.232) train_loss=380.75433350 time/batch=22.28s
setting learning rate to 0.0003898
352/485 (epoch 72.437) train_loss=305.50891113 time/batch=17.41s
353/485 (epoch 72.643) train_loss=548.29870605 time/batch=31.42s
354/485 (epoch 72.849) train_loss=766.80627441 time/batch=63.49s
355/485 (epoch 73.055) train_loss=673.09448242 time/batch=186.65s
setting learning rate to 0.0003781
356/485 (epoch 73.260) train_loss=320.52062988 time/batch=17.68s
357/485 (epoch 73.466) train_loss=936.35290527 time/batch=187.12s
358/485 (epoch 73.672) train_loss=577.10760498 time/batch=32.51s
359/485 (epoch 73.878) train_loss=465.41983032 time/batch=25.24s
setting learning rate to 0.0003668
360/485 (epoch 74.084) train_loss=397.31494141 time/batch=21.86s
361/485 (epoch 74.289) train_loss=780.30236816 time/batch=64.80s
362/485 (epoch 74.495) train_loss=495.18225098 time/batch=26.07s
363/485 (epoch 74.701) train_loss=400.02917480 time/batch=27.04s
setting learning rate to 0.0003557
364/485 (epoch 74.907) train_loss=935.93444824 time/batch=188.13s
365/485 (epoch 75.113) train_loss=307.44805908 time/batch=18.20s
366/485 (epoch 75.318) train_loss=576.09399414 time/batch=32.76s
367/485 (epoch 75.524) train_loss=444.08813477 time/batch=24.26s
setting learning rate to 0.0003451
368/485 (epoch 75.730) train_loss=424.12475586 time/batch=22.94s
369/485 (epoch 75.936) train_loss=556.21740723 time/batch=31.98s
370/485 (epoch 76.141) train_loss=897.27478027 time/batch=141.55s
371/485 (epoch 76.347) train_loss=346.24822998 time/batch=20.92s
setting learning rate to 0.0003347
372/485 (epoch 76.553) train_loss=460.82409668 time/batch=27.68s
373/485 (epoch 76.759) train_loss=554.50207520 time/batch=35.49s
374/485 (epoch 76.965) train_loss=732.78045654 time/batch=67.34s
375/485 (epoch 77.170) train_loss=357.92913818 time/batch=18.60s
setting learning rate to 0.0003247
376/485 (epoch 77.376) train_loss=313.55584717 time/batch=18.03s
377/485 (epoch 77.582) train_loss=423.89556885 time/batch=22.98s
378/485 (epoch 77.788) train_loss=524.80712891 time/batch=29.55s
379/485 (epoch 77.994) train_loss=666.66290283 time/batch=47.08s
setting learning rate to 0.0003149
380/485 (epoch 78.199) train_loss=307.81216431 time/batch=18.10s
381/485 (epoch 78.405) train_loss=410.38360596 time/batch=24.39s
382/485 (epoch 78.611) train_loss=491.01586914 time/batch=27.13s
383/485 (epoch 78.817) train_loss=939.28668213 time/batch=199.50s
setting learning rate to 0.0003055
384/485 (epoch 79.023) train_loss=551.39605713 time/batch=34.80s
385/485 (epoch 79.228) train_loss=390.42520142 time/batch=22.48s
386/485 (epoch 79.434) train_loss=933.95269775 time/batch=211.44s
387/485 (epoch 79.640) train_loss=373.77493286 time/batch=22.93s
setting learning rate to 0.0002963
388/485 (epoch 79.846) train_loss=496.70526123 time/batch=26.52s
389/485 (epoch 80.051) train_loss=372.57745361 time/batch=19.68s
390/485 (epoch 80.257) train_loss=763.12518311 time/batch=64.16s
391/485 (epoch 80.463) train_loss=333.49407959 time/batch=22.34s
setting learning rate to 0.0002874
392/485 (epoch 80.669) train_loss=425.00601196 time/batch=23.48s
393/485 (epoch 80.875) train_loss=602.85168457 time/batch=37.98s
394/485 (epoch 81.080) train_loss=742.95361328 time/batch=73.72s
395/485 (epoch 81.286) train_loss=306.00735474 time/batch=17.85s
setting learning rate to 0.0002788
396/485 (epoch 81.492) train_loss=373.35864258 time/batch=19.90s
397/485 (epoch 81.698) train_loss=331.70617676 time/batch=22.16s
398/485 (epoch 81.904) train_loss=589.64501953 time/batch=34.67s
399/485 (epoch 82.109) train_loss=488.90640259 time/batch=35.63s
setting learning rate to 0.0002705
  saved to metadata
